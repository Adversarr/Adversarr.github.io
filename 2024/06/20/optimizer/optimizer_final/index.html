<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Optimization Lecture 2: Convex Set Common Convex Sets  Hyperplane: \(H &#x3D; \{x \in \mathbb{R}^n \mid a^Tx &#x3D; b\}\). Halfspace: \(H &#x3D; \{x \in \mathbb{R}^n \mid a^Tx \leq b\}\). Polyhedron: \(P &#x3D; \{x \in \">
<meta property="og:type" content="article">
<meta property="og:title" content="A short summary to Optimization Theory.">
<meta property="og:url" content="https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/index.html">
<meta property="og:site_name" content="Adversarr&#39;s Blog">
<meta property="og:description" content="Optimization Lecture 2: Convex Set Common Convex Sets  Hyperplane: \(H &#x3D; \{x \in \mathbb{R}^n \mid a^Tx &#x3D; b\}\). Halfspace: \(H &#x3D; \{x \in \mathbb{R}^n \mid a^Tx \leq b\}\). Polyhedron: \(P &#x3D; \{x \in \">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-06-20T16:00:00.000Z">
<meta property="article:modified_time" content="2024-06-22T10:59:59.333Z">
<meta property="article:author" content="Adversarr">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/magic-wand-48.png">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/magic-wand-192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/magic-wand-180.png">
        
      
    
    <!-- title -->
    <title>A short summary to Optimization Theory.</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$'], ['\\(', '\\)']]
			}
		  });
		</script>
		<script src='https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js' async></script>
	
<meta name="generator" content="Hexo 7.0.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/adversarr">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/06/18/linsys/spai/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&text=A short summary to Optimization Theory."><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&is_video=false&description=A short summary to Optimization Theory."><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=A short summary to Optimization Theory.&body=Check out this article: https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&name=A short summary to Optimization Theory.&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&t=A short summary to Optimization Theory."><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#optimization"><span class="toc-number">1.</span> <span class="toc-text">Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-2-convex-set"><span class="toc-number">1.1.</span> <span class="toc-text">Lecture 2: Convex Set</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#common-convex-sets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Common Convex Sets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#property-of-conv"><span class="toc-number">1.1.2.</span> <span class="toc-text">Property of conv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#operations-on-convex-sets"><span class="toc-number">1.1.3.</span> <span class="toc-text">Operations on Convex Sets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dual-cone-generalized-inequalities"><span class="toc-number">1.1.4.</span> <span class="toc-text">Dual Cone &amp; Generalized
Inequalities</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#separating-hyperplane-theorem"><span class="toc-number">1.1.5.</span> <span class="toc-text">Separating Hyperplane
Theorem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-3-convex-function"><span class="toc-number">1.2.</span> <span class="toc-text">Lecture 3: Convex Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gradients-hessians"><span class="toc-number">1.2.1.</span> <span class="toc-text">Gradients &amp; Hessians</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#definition"><span class="toc-number">1.2.2.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checking-convexity"><span class="toc-number">1.2.3.</span> <span class="toc-text">checking convexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#operations"><span class="toc-number">1.2.4.</span> <span class="toc-text">Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-4-convex-problems"><span class="toc-number">1.3.</span> <span class="toc-text">Lecture 4: Convex Problems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-5-optimal-conditions"><span class="toc-number">1.4.</span> <span class="toc-text">Lecture 5: Optimal conditions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#first-order-condition-and-second-order-condition"><span class="toc-number">1.4.1.</span> <span class="toc-text">First Order
Condition and Second Order Condition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#duality"><span class="toc-number">1.4.2.</span> <span class="toc-text">Duality</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimal-condition-slater-kkt-licq"><span class="toc-number">1.4.3.</span> <span class="toc-text">Optimal condition: Slater,
KKT, LICQ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#general-kkt-condition"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">General KKT condition</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-6-gradient-descent"><span class="toc-number">1.5.</span> <span class="toc-text">Lecture 6: Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#more-on-convex-functions"><span class="toc-number">1.5.1.</span> <span class="toc-text">More on convex functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#line-searching"><span class="toc-number">1.5.2.</span> <span class="toc-text">Line searching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#non-smooth-problems"><span class="toc-number">1.5.3.</span> <span class="toc-text">Non smooth problems</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-7-subgradients"><span class="toc-number">1.6.</span> <span class="toc-text">Lecture 7: Subgradients</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-of-subgradients-subdifferential"><span class="toc-number">1.6.1.</span> <span class="toc-text">Definition of
Subgradients &amp; Subdifferential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#computing-subgradients-subdifferential"><span class="toc-number">1.6.2.</span> <span class="toc-text">Computing Subgradients
&amp; Subdifferential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimal-condition"><span class="toc-number">1.6.3.</span> <span class="toc-text">Optimal condition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#subgradient-methods"><span class="toc-number">1.6.4.</span> <span class="toc-text">Subgradient methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-8-projected-gradient-descent-conditional-gradient-descent"><span class="toc-number">1.7.</span> <span class="toc-text">Lecture
8: projected gradient descent, conditional gradient descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-gradient-descent"><span class="toc-number">1.7.1.</span> <span class="toc-text">Conditional Gradient Descent</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-9-prox-mapping"><span class="toc-number">1.8.</span> <span class="toc-text">Lecture 9: Prox Mapping</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-of-proximal-mapping"><span class="toc-number">1.8.1.</span> <span class="toc-text">Definition of Proximal
Mapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#moreau-decomposition"><span class="toc-number">1.8.2.</span> <span class="toc-text">Moreau decomposition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#proximal-gradient-descent"><span class="toc-number">1.8.3.</span> <span class="toc-text">Proximal Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#convergency"><span class="toc-number">1.8.3.1.</span> <span class="toc-text">Convergency</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#moreau-envelope"><span class="toc-number">1.8.4.</span> <span class="toc-text">Moreau Envelope</span></a></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        A short summary to Optimization Theory.
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Adversarr</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-06-20T16:00:00.000Z" class="dt-published" itemprop="datePublished">2024-06-21</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="optimization">Optimization</h1>
<h2 id="lecture-2-convex-set">Lecture 2: Convex Set</h2>
<h3 id="common-convex-sets">Common Convex Sets</h3>
<ul>
<li><strong>Hyperplane</strong>: <span class="math inline">\(H = \{x \in
\mathbb{R}^n \mid a^Tx = b\}\)</span>.</li>
<li><strong>Halfspace</strong>: <span class="math inline">\(H = \{x \in
\mathbb{R}^n \mid a^Tx \leq b\}\)</span>.</li>
<li><strong>Polyhedron</strong>: <span class="math inline">\(P = \{x \in
\mathbb{R}^n \mid Ax \leq b\}\)</span>.</li>
<li><strong>Euclidean Ball</strong>: <span class="math inline">\(B = \{x
\in \mathbb{R}^n \mid \|x - x_c\|_2 \leq r\}\)</span>.</li>
<li><strong>Ellipsoid</strong>: <span class="math inline">\(E = \{x \in
\mathbb{R}^n \mid (x - x_c)^T P^{-1} (x - x_c) \leq 1\}\)</span>.</li>
<li><strong>Norm Ball</strong>: <span class="math inline">\(B = \{x \in
\mathbb{R}^n \mid \|x - x_c\|_p \leq r\}\)</span>.</li>
<li><strong>Norm Cone</strong>: <span class="math inline">\(C = \{x \in
\mathbb{R}^n \mid \|x\|_p \leq t\}\)</span>.</li>
</ul>
<p>Some special matrices:</p>
<ul>
<li>Symmetric matrix set: <span class="math inline">\(S^n = \{X \in
\mathbb{R}^{n \times n} \mid X = X^T\}\)</span>.</li>
<li>Semi-positive definite matrix set: <span class="math inline">\(S_+^n
= \{X \in \mathbb{R}^{n \times n} \mid X = X^T, \lambda_{\min}(X) \geq
0\}\)</span>.</li>
<li>Positive definite matrix set: <span class="math inline">\(S_{++}^n =
\{X \in \mathbb{R}^{n \times n} \mid X = X^T, \lambda_{\min}(X) &gt;
0\}\)</span>.</li>
</ul>
<p>Actually, SPD matrix set is a cone.</p>
<h3 id="property-of-conv">Property of <code>conv</code></h3>
<p>Convex combination: <span class="math display">\[
x = \theta_1 x_1 + ... + \theta_k x_k, \theta_i \geq 0, \sum_{i=1}^k
\theta_i = 1.
\]</span></p>
<p><span class="math inline">\(\mathbf{conv} S\)</span> is the smallest
convex set that contains <span class="math inline">\(S\)</span>. - Also
the intersection of all convex sets that contain <span
class="math inline">\(S\)</span>.</p>
<p><code>aff</code>: The smallest affine set that contains <span
class="math inline">\(S\)</span>. - The smallest affine set that
contains <span class="math inline">\(S\)</span>. - Containing all the
affine combinations of points in <span
class="math inline">\(S\)</span>.</p>
<p>Cone: <span class="math display">\[
C = \{x \in \mathbb{R}^n \mid \lambda x \in C, \forall \lambda \geq 0\}
\]</span></p>
<p>Cone combination: <span class="math display">\[
x = \theta_1 x_1 + ... + \theta_k x_k, \theta_i \geq 0, x_i \in C.
\]</span></p>
<ul>
<li>If all the cone combinations of <span class="math inline">\(x_1,
..., x_k\)</span> are in <span class="math inline">\(C\)</span>, then
<span class="math inline">\(C\)</span> is a convex cone.</li>
</ul>
<h3 id="operations-on-convex-sets">Operations on Convex Sets</h3>
<p>Affine transform keeps convexity.</p>
<p>Perspective transform <span class="math inline">\(P: R^{n+1} \to
R^n\)</span> <span class="math display">\[
P(x, t) = \frac{x}{t}, \text{dom} P = \{(x, t) \mid t &gt; 0\}
\]</span> Perspective transform keeps convexity.</p>
<p>Therefore: <span class="math display">\[
f(x)  = \frac{Ax + b}{c^Tx + d}, \text{dom} f = \{x \mid c^Tx + d &gt;
0\}
\]</span> keeps convexity.</p>
<h3 id="dual-cone-generalized-inequalities">Dual Cone &amp; Generalized
Inequalities</h3>
<p><strong>Proper cone</strong>: - Closed - <span
class="math inline">\(\text{int} K\)</span> is nonempty - <span
class="math inline">\(K\)</span> is pointed: <span
class="math inline">\(K \cap -K = \{0\}\)</span></p>
<p>For example: 1. Nonnegative orthant: <span class="math inline">\(K =
\mathbb{R}_+^n\)</span> 2. Semi-PD matrix cone: <span
class="math inline">\(K = S_{+}^n\)</span></p>
<p><strong>Generalized Inequalities</strong>: For proper cone <span
class="math inline">\(K\)</span>, <span class="math display">\[
x \preceq_K y \Leftrightarrow y - x \in K
\]</span> Strict inequality: <span class="math display">\[
x \prec_K y \Leftrightarrow y - x \in \text{int} K
\]</span></p>
<p><strong>Dual Cone</strong>: For <span
class="math inline">\(K\)</span>, the dual cone is <span
class="math display">\[
K^* = \{y \mid x^Ty \geq 0, \forall x \in K\}
\]</span></p>
<p>There are many properties for dual cone</p>
<ul>
<li><span class="math inline">\(K^*\)</span> is a cone, and a closed
convex set.</li>
<li>if <span class="math inline">\(\mathrm{int} K\)</span> is nonempty,
then <span class="math inline">\(K^*\)</span> is pointed</li>
<li>… (see slide page 36.)</li>
</ul>
<p>One important property is: if <span class="math inline">\(K\)</span>
is proper, then <span class="math inline">\(K^*\)</span> is also
proper.</p>
<p>There is dual generalized inequalities: <span class="math display">\[
x \preceq_K y \iff y - x \in K^*
\]</span> it satisfies: - <span class="math inline">\(x \preceq_K y
\implies z^Tx \leq z^Ty, \forall z \succeq_{K^*} 0\)</span></p>
<blockquote>
<p>The advantage of dual generalized inequalities is that, the dual cone
is always closed and convex, and it converts a partial order to a full
order problem.</p>
</blockquote>
<blockquote>
<p>Reference: 02-convex-set-ustc.pdf, page 37</p>
</blockquote>
<h3 id="separating-hyperplane-theorem">Separating Hyperplane
Theorem</h3>
<p>For two disjoint convex sets <span class="math inline">\(C\)</span>
and <span class="math inline">\(D\)</span>, there exists a hyperplane
that separates them.</p>
<p><strong>Supporting Hyperplane</strong>: For a convex set <span
class="math inline">\(C\)</span>, and a point <span
class="math inline">\(x_0 \in \partial C\)</span>, there exists a
hyperplane <span class="math inline">\(a^Tx = b\)</span> such that <span
class="math inline">\(a^Tx \leq b, \forall x \in C\)</span>.</p>
<h2 id="lecture-3-convex-function">Lecture 3: Convex Function</h2>
<h3 id="gradients-hessians">Gradients &amp; Hessians</h3>
<p><strong>Gradient</strong>: <span class="math display">\[
\lim_{p\to 0} \frac{f(x + p) - f(x)}{\|p\|} = \nabla f(x)
\]</span></p>
<p><strong>Hessian</strong>: <span class="math display">\[
\nabla^2 f(x) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial
x_1 \partial x_2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_1
\partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2
f}{\partial x_2^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_2
\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2
f}{\partial x_n \partial x_2} &amp; \cdots &amp; \frac{\partial^2
f}{\partial x_n^2}
\end{bmatrix}
\]</span></p>
<p>Gateaux derivative: <span class="math display">\[
\lim_{\alpha \to 0} \frac{f(x + \alpha d) - f(x) - t \langle g, d
\rangle}{\alpha}
\]</span></p>
<p>Some important derivatives: <span class="math display">\[
\text{Linear: } \nabla_x \mathrm{tr} (A X^T B) = BA
\]</span></p>
<p><span class="math display">\[
\text{Quadratic: } \nabla_x x^T A x = (A + A^T) x
\]</span></p>
<p><span class="math display">\[
\nabla \ln \det X = X^{-1}
\]</span></p>
<h3 id="definition">Definition</h3>
<p>A function <span class="math inline">\(f: \mathbb{R}^n \to
\mathbb{R}\)</span> is convex if: <span class="math display">\[
f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta)f(y), \forall x,
y \in \text{dom} f, \theta \in [0, 1]
\]</span></p>
<p>Strict convex: <span class="math display">\[
f(\theta x + (1-\theta)y) &lt; \theta f(x) + (1-\theta)f(y), \forall x,
y \in \text{dom} f, \theta \in (0, 1)
\]</span></p>
<p>some important examples in 1D: - Affines - Exponentials - Powers
<span class="math inline">\(x^p\)</span>, with <span
class="math inline">\(p \geq 1, x \in \mathbb R_{++}\)</span> - Negative
entropy <span class="math inline">\(x \ln x\)</span>, with <span
class="math inline">\(x \in \mathbb R_{++}\)</span></p>
<p>basic examples in <span class="math inline">\(\mathbb{R}^n\)</span>:
- Affine functions - Norm, with <span class="math inline">\(p \geq
1\)</span></p>
<p>In matrix space: - <span class="math inline">\(f(X) = \mathrm{tr}
(AX) = \sum_{ij} A_{ij} X_{ij} + b\)</span> is affine, convex. - Matrix
norms (<span class="math inline">\(p=2\)</span>): <span
class="math inline">\(f(X) = \|X\|_2\)</span> is convex.</p>
<h3 id="checking-convexity">checking convexity</h3>
<p><strong>Restrict to line</strong>: <span class="math inline">\(g(t) =
f(x + td)\)</span> is always convex, then <span
class="math inline">\(f\)</span> is convex.</p>
<p><strong>First order condition</strong>: if <span
class="math inline">\(f\)</span> is differentiable, then <span
class="math inline">\(f\)</span> is convex if and only if <span
class="math inline">\(f(y) \geq f(x) + \nabla f(x)^T (y -
x)\)</span>.</p>
<p>More on derivative: if <span class="math inline">\(f\)</span> is
differentiable, then <span class="math inline">\(f\)</span> is convex if
and only if <span class="math inline">\(\mathrm{dom} f\)</span> is
convex, and <span class="math inline">\(\nabla f\)</span> is
monotonically increasing: <span class="math display">\[
(\nabla f(x) - \nabla f(y))^T (x - y) \geq 0, \forall x, y \in
\text{dom} f
\]</span></p>
<p><strong>Second order condition</strong>: if <span
class="math inline">\(f\)</span> is twice differentiable, then <span
class="math inline">\(f\)</span> is convex if and only if <span
class="math inline">\(\nabla^2 f(x) \succeq 0, \forall x \in \text{dom}
f\)</span>.</p>
<p>And if <span class="math inline">\(\nabla^2 f \succ 0\)</span>, then
<span class="math inline">\(f\)</span> is strictly convex.</p>
<p><strong>Jensen’s Inequality</strong>: if <span
class="math inline">\(f\)</span> is convex, then 1. <span
class="math inline">\(f(\theta x + (1-\theta)y) \leq \theta f(x) +
(1-\theta)f(y)\)</span> 2. <span class="math inline">\(f(\mathbb{E}[X])
\leq \mathbb{E}[f(X)]\)</span></p>
<h3 id="operations">Operations</h3>
<p><strong>Nonnegative weighted sum</strong>: <span
class="math display">\[
f(x) = \sum_{i=1}^k \theta_i f_i(x), \theta_i \geq 0
\]</span></p>
<p><strong>Pointwise maximum</strong>: <span class="math display">\[
f(x) = \max_{i=1}^k f_i(x)
\]</span></p>
<p><strong>Composition with affine transform</strong>: <span
class="math display">\[
f(x) = g(Ax + b)
\]</span></p>
<p><strong>Composition with perspective transform</strong>: <span
class="math display">\[
g(x, t) = t f(x / t)
\]</span></p>
<p><strong>Conjugate function</strong>: <span class="math display">\[
f^*(y) = \sup_{x \in \text{dom} f} \langle x, y \rangle - f(x)
\]</span></p>
<ul class="task-list">
<li><label><input type="checkbox" />Connection to proximal operator,
Moreau decomposition.</label></li>
</ul>
<h2 id="lecture-4-convex-problems">Lecture 4: Convex Problems</h2>
<p><strong>Definition of Convex Optimization Problem</strong>: <span
class="math display">\[
\begin{aligned}
       \min \quad &amp;f_0(x) \\
\text{s.t.} \quad &amp;f_i(x) \leq 0, i = 1, ..., m \\
                  &amp;h_i(x) = 0, i = 1, ..., p
\end{aligned}
\]</span></p>
<p>basic property: the feasible set is convex.</p>
<p><strong>Theorem</strong>: Any local minimum of a convex problem is a
global minimum.</p>
<p><strong>Linear Programming</strong>: <span class="math display">\[
\min c^Tx \quad \text{s.t.} \quad Ax = b, G x \leq e
\]</span> and fractional linear programming, is also equivalent to
linear programming.</p>
<p><strong>Quadratic Programming</strong>: <span class="math display">\[
\min \frac{1}{2} x^T P x + q^T x + r \quad \text{s.t.} \quad Ax = b, G x
\leq e
\]</span></p>
<p>some examples: - Least squares - Stotasitc Linear Programming</p>
<p><strong>QCQP</strong>: <span class="math display">\[
\min x^T P_0 x + q_0^T x + r_0 \quad \text{s.t.} \quad x^T P_i x + q_i^T
x + r_i \leq 0, i = 1, ..., m
\]</span></p>
<p><strong>SOCP</strong>: <span class="math display">\[
\begin{aligned}
       \min \quad &amp;c^Tx \\
\text{s.t.} \quad &amp;\|A_i x + b_i\|_2 \leq c_i^T x + d_i, i = 1, ...,
m
                  &amp;Fx = g
\end{aligned}
\]</span></p>
<p><strong>Convex Relaxation</strong>: max-cut: <span
class="math display">\[
\begin{aligned}
       \max \quad &amp;\frac{1}{2} \sum_{i &lt; j} (1 - x_i x_j) w_{ij}
\\
\text{s.t.} \quad &amp;x_i \in \{-1, 1\}
\end{aligned}
\]</span></p>
<p>relaxation: let <span class="math inline">\(W = (w_{ij}) \in
S^n\)</span>, <span class="math inline">\(C = -\frac{1}{4}
(\mathrm{diag} (W 1) - W)\)</span> (graph laplacian): <span
class="math display">\[
\begin{aligned}
       \max \quad &amp;\frac{1}{4} x^T C x \\
\text{s.t.} \quad &amp;x_i \in \{-1, 1\}
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(X = xx^T\)</span>, and use <span
class="math inline">\(x_i^2 = 1\)</span>, we have: <span
class="math display">\[
\begin{aligned}
       \max \quad &amp;\frac{1}{4} \mathrm{tr} (CX) \\
\text{s.t.} \quad &amp;X \succeq 0, X_{ii} = 1, X\preceq 0,
\mathrm{rank} X = 1
\end{aligned}
\]</span></p>
<h2 id="lecture-5-optimal-conditions">Lecture 5: Optimal conditions</h2>
<h3 id="first-order-condition-and-second-order-condition">First Order
Condition and Second Order Condition</h3>
<p><strong>First order</strong>: Suppose <span
class="math inline">\(f\)</span> is differentiable, then <span
class="math inline">\(x\)</span> is optimal must satisfies <span
class="math display">\[
\nabla f(x) = 0
\]</span> This is only a necessary condition generally. But for convex
function, it is also sufficient.</p>
<p><strong>Second order</strong>: Suppose <span
class="math inline">\(f\)</span> is twice differentiable, 1. necessary
condition: <span class="math inline">\(\nabla f(x) = 0, \nabla^2 f
\preceq 0\)</span> 2. sufficient condition: <span
class="math inline">\(\nabla f(x) = 0, \nabla^2 f \prec 0\)</span></p>
<h3 id="duality">Duality</h3>
<p><strong>Lagrange Dual Function</strong>: for a convex optimization
problem <span class="math display">\[
\begin{aligned}
       \min \quad &amp;f_0(x) \\
\text{s.t.} \quad &amp;f_i(x) \leq 0, i = 1, ..., m \\
                  &amp;h_i(x) = 0, i = 1, ..., p
\end{aligned}
\]</span> the Lagrange dual function is <span class="math display">\[
L(x, \lambda, \nu) = f_0(x) + \sum_{i=1}^m \lambda_i f_i(x) +
\sum_{i=1}^p \nu_i h_i(x)
\]</span></p>
<p><strong>Weak Duality</strong>: for any feasible <span
class="math inline">\(x\)</span> and <span
class="math inline">\(\lambda, \nu\)</span>, <span
class="math display">\[
g(\lambda, \nu) = \inf_x L(x, \lambda, \nu)
\]</span> and, if <span class="math inline">\(\lambda \ge 0\)</span>, we
have: <span class="math display">\[
g(\lambda, \nu) \leq p^*
\]</span></p>
<p><strong>Lagrange Dual Problem</strong>: the dual problem is: <span
class="math display">\[
\begin{aligned}
       \max \quad &amp;g(\lambda, \nu) = \inf_x L(x, \lambda, \nu) \\
\text{s.t.} \quad &amp;\lambda \succeq 0
\end{aligned}
\]</span> and duality gap is <span class="math inline">\(p^* - d^* \ge
0\)</span>, where <span class="math inline">\(p^*\)</span> is the primal
optimal value, and <span class="math inline">\(d^*\)</span> is the dual
optimal value.</p>
<p>Duality feasible: <span class="math display">\[
\mathrm{dom} g = \{(\lambda, \nu) \mid \lambda \succeq 0, g(\lambda,
\nu) &gt; -\infty\}
\]</span></p>
<p>Examples: - LP -&gt; LP - QP -&gt; QP</p>
<p><strong>Connection to conjugate function</strong>: if the constraint
is linear, <span class="math display">\[
g(\lambda, \nu) = -f_0^*(-A^T \lambda - C^T \nu) - b^T \lambda - d^T \nu
\]</span> When conjugate function is known, we can directly solve the
dual problem.</p>
<p><strong>Change of variables</strong>: <span class="math display">\[
\min f_0 (Ax + b)
\]</span> can be changed to: <span class="math display">\[
\min f_0(y) \quad \text{s.t.} \quad y = Ax + b
\]</span></p>
<p><strong>Bound constrainted LP</strong>: <span class="math display">\[
\begin{aligned}
        \min &amp;c^T x \\
\text{s.t.} &amp;A x = b, -1 \le x \le 1
\end{aligned}
\]</span></p>
<p>Implicit boundary constraint: <span class="math display">\[
\begin{aligned}
        \min &amp;f_0(x) = \begin{cases}
        c^T x &amp;\text{if } -1 \le x \le 1 \\
        +\infty &amp;\text{otherwise}
    \end{cases} \\
\text{s.t.} &amp;A x = b, -1 \le x \le 1
\end{aligned}
\]</span> the dual function: <span class="math display">\[
g(\nu) = -b^T \nu - \| A^T \nu + c \|_1
\]</span></p>
<p><strong>Cones</strong>: How to deal with cone constraint? <span
class="math display">\[
\begin{aligned}
       \min &amp; f(x) \\
\text{s.t.} &amp; c_i(x) \succeq_{K_i} 0\\
            &amp; d_i(x) = 0
\end{aligned}
\]</span> the Lagrange dual function is: <span class="math display">\[
L(x, \lambda, \nu) = f(x) + \sum_i \langle \lambda_i, c_i(x)\rangle +
\sum_i \nu_i^T d_i(x)
\]</span></p>
<h3 id="optimal-condition-slater-kkt-licq">Optimal condition: Slater,
KKT, LICQ</h3>
<p>Why we need slater’s condition: wish to find the optimal condition
for a constrainted optimization problem, as if we are solving an
unconstrainted problem.</p>
<p><strong>Slater’s Condition</strong>: if there exists a feasible <span
class="math inline">\(x \in \mathrm{relint} \mathcal D\)</span> such
that <span class="math inline">\(f_i(x) &lt; 0, h_i(x) = 0\)</span>,
then we say slate’s condition holds.</p>
<p>For convex problem, Slater’s condition is sufficient for
<strong>strong duality</strong>(zero duality gap).</p>
<p><strong>KKT condition</strong>, also the <strong>First order
sufficient and necessary condition</strong>: For a convex optimization
problem, if Slater’s condition holds, then the KKT condition is
necessary and sufficient for optimality: 1. stationary: <span
class="math inline">\(0 \in \partial f(x^*) + \sum \lambda_i \partial
f_i(x^*) + \sum \nu_i \partial h_i(x^*)\)</span> 2. primal feasibility:
<span class="math inline">\(f_i(x^*) \le 0, h_i(x^*) = 0\)</span> 3.
duality feasibility: <span class="math inline">\(\lambda_i \ge
0\)</span> 4. complementary slackness: <span
class="math inline">\(\lambda_i f_i(x^*) = 0\)</span></p>
<h4 id="general-kkt-condition">General KKT condition</h4>
<p><strong>LICQ</strong>(Linear Independence Constraint Qualification):
for a point <span class="math inline">\(x\)</span>, if the gradients of
active constraints are linearly independent, then we say LICQ holds.</p>
<p>For a general problem, we need LICQ condition to ensure the KKT
condition is necessary and sufficient.</p>
<blockquote>
<p>TODO: More on LICQ</p>
</blockquote>
<ul>
<li>Analytic solution for some basic problems.</li>
</ul>
<h2 id="lecture-6-gradient-descent">Lecture 6: Gradient Descent</h2>
<p>For a quadratic problem: <span class="math display">\[
\| x_k - x^* \|^2 \le \left(\frac{\lambda_{\max} -
\lambda_{\min}}{\lambda_{\max} + \lambda_{\min}}\right) \|x_0 - x^*\|^2
\]</span></p>
<p>Therefore, we consider the Lipschitz constant: <span
class="math display">\[
\| \nabla f(x) - \nabla f(y) \| \le L \|x - y\|
\]</span> the Lipschitz constant also gives us: <span
class="math display">\[
\begin{aligned}
    f(y) &amp;\le f(x) + \nabla f(x)^T (y - x) + \frac{L}{2} \|y - x\|^2
\\
    f(x) &amp;\ge f(y) + \nabla f(y)^T (x - y) + \frac{L}{2} \|x - y\|^2
\end{aligned}
\]</span> To ensure convergence, we must set <span
class="math inline">\(\alpha_k &lt; \frac{1}{L}\)</span>.</p>
<p>Moreover, if <span class="math inline">\(f\)</span> is twice
differentiable, then <span class="math inline">\(f\)</span> is <span
class="math inline">\(m\)</span>-strong convex, and <span
class="math inline">\(L\)</span>-continuously differentiable, if: <span
class="math display">\[
m I \preceq \nabla^2 f(x) \preceq L I
\]</span></p>
<p>The convergence rate of GD is given by: <span class="math display">\[
f(x_k) - f(x^*) \le \frac{L \|x_0 - x^*\|^2}{2k}
\]</span></p>
<p>Convergence rate under strong convexity: <span
class="math display">\[
f(x^k) - f^* \le c^k \frac{L}{2} \| x^0 - x^* \|_2^2
\]</span> where <span class="math inline">\(c = L / m &lt;
1\)</span>.</p>
<h3 id="more-on-convex-functions">More on convex functions</h3>
<p>For a differentiable function, tfae: 1. <span
class="math inline">\(\nabla f\)</span> is Lipschitz continuous with
constant <span class="math inline">\(L\)</span>, 2. <span
class="math inline">\(g(x) := L/2 \|x\|_2^2 - f(x)\)</span> is convex,
3. <span class="math inline">\(\nabla f\)</span> has: <span
class="math inline">\((\nabla f(x) - \nabla f(y))^T (x - y) \ge L \|x -
y\|^2\)</span>, for all <span class="math inline">\(x, y\)</span></p>
<h3 id="line-searching">Line searching</h3>
<p><strong>Armijo rule</strong>, checking for sufficient decrease: <span
class="math display">\[
f(x_k + \alpha_k d_k) \le f(x_k) + \alpha_k c_1 \nabla f(x_k)^T d_k
\]</span></p>
<p><strong>Wolfe conditions</strong>, checking for curvature: <span
class="math display">\[
\begin{aligned}
    f(x_k + \alpha_k d_k) &amp;\le f(x_k) + \alpha_k c_1 \nabla f(x_k)^T
d_k \\
    \nabla f(x_k + \alpha_k d_k)^T d_k &amp;\ge c_2 \nabla f(x_k)^T d_k
\end{aligned}
\]</span> we need <span class="math inline">\(0 &lt; c_1 &lt; c_2 &lt;
1\)</span>.</p>
<p><strong>Goldstein conditions</strong>, a compromise between Armijo
and Wolfe: <span class="math display">\[
\begin{aligned}
    f(x_k + \alpha_k d_k) &amp;\le f(x_k) + \alpha_k c_1 \nabla f(x_k)^T
d_k \\
    f(x_k + \alpha_k d_k) &amp;\ge f(x_k) + \alpha_k (1-c) \nabla
f(x_k)^T d_k
\end{aligned}
\]</span> we need <span class="math inline">\(0 &lt; c &lt;
1/2\)</span></p>
<h3 id="non-smooth-problems">Non smooth problems</h3>
<p>Convergence rate for gd: <span class="math display">\[
\min _{k = 0, 1, ..., T} \| \nabla f (x_k) \|^2 \le \frac{2 L (f(x_0) -
f^*)}{T}
\]</span></p>
<p>TODO: Check for the convergency proof.</p>
<h2 id="lecture-7-subgradients">Lecture 7: Subgradients</h2>
<h3 id="definition-of-subgradients-subdifferential">Definition of
Subgradients &amp; Subdifferential</h3>
<p><strong>Subgradient</strong>: <span class="math display">\[
\partial f(x) = \{ g \mid f(y) \ge f(x) + g^T (y - x), \forall y \}
\]</span></p>
<p>For a convex function, <span class="math inline">\(x \in
\mathrm{int~dom} f\)</span>, the subgradient is always nonempty.</p>
<h3 id="computing-subgradients-subdifferential">Computing Subgradients
&amp; Subdifferential</h3>
<p>differentiable function: equivalent to strong derivative.</p>
<p>Non-negative weighted sum.</p>
<p>Composition with affine transform.</p>
<p>Pointwise maximum: <span class="math display">\[
f(x) = \max \{ f_1(x), f_2(x) \}
\implies
\partial f(x) = \mathbf{conv} \partial f_1(x) \cap \partial f_2(x)
\]</span></p>
<p>Examples: - Piecewise linear function - L1 norm.</p>
<h3 id="optimal-condition">Optimal condition</h3>
<p><strong>First order condition</strong>(unconstrainted): <span
class="math inline">\(x^*\)</span> is local minimum, if and only if,
<span class="math display">\[
0 \in \partial f
\]</span></p>
<p><strong>First order condition</strong>(constrainted): <span
class="math inline">\(x^*\)</span> is local minimum, if and only if,
<span class="math display">\[
0 \in \partial f(x^*) + \sum \lambda_i \partial f_i(x^*)
\]</span> where <span class="math inline">\(\lambda_i \ge 0\)</span> is
the optimal dual variable.</p>
<h3 id="subgradient-methods">Subgradient methods</h3>
<p><span class="math display">\[
x^{k+1}\larr x^k - \alpha_k g^k
\]</span> we have many choices for <span
class="math inline">\(\alpha_k\)</span>: 1. constant step size: does not
guarantee convergency 2. constant <span class="math inline">\(\|x^{k+1}
- x^k\|\)</span>: does not guarantee convergency 3. diminishing step
size: ok, a typical choice is <span class="math inline">\(\alpha_k =
1/k\)</span> 4. line search: ok, but expensive.</p>
<h2
id="lecture-8-projected-gradient-descent-conditional-gradient-descent">Lecture
8: projected gradient descent, conditional gradient descent</h2>
<p>Now consider constrainted problems.</p>
<p><strong>Optimal condition</strong>: <span class="math display">\[
\langle \nabla f(x^*), x^* - y\rangle = 0,\quad \forall y in C
\]</span></p>
<p><strong>Projected Gradient Descent</strong>: <span
class="math display">\[
x_{k+1} = \Pi_C (x_k - \alpha_k \nabla f(x_k))
\]</span></p>
<p><strong>Non expansion property of projection</strong>: if <span
class="math inline">\(C\)</span> is convex, <span
class="math display">\[
\| \Pi_C(x) - \Pi_C(y) \| \le \|x - y\|
\]</span></p>
<p>The convergency result under strong convex assumption: <span
class="math display">\[
\|x_k - x^*\|^2 \leq \left(1 - \frac{m}{L}\right)^k \|x_0 - x^*\|^2
\]</span> if we choose <span class="math inline">\(\alpha_k =
1/L\)</span></p>
<h3 id="conditional-gradient-descent">Conditional Gradient Descent</h3>
<p>also called Frank-Wolfe algorithm.</p>
<p>Why we need conditional gradient descent: Projection is expensive</p>
<p><strong>Conditional Gradient Descent</strong>: <span
class="math display">\[
\begin{aligned}
    x_k &amp;= \arg\min_{x \in C} \langle \nabla f(x_{k-1}), x \rangle,
\\
    y_k &amp;= x_{k-1} + \alpha_k (x_k - x_{k-1})
\end{aligned}
\]</span> might be easier, if the linear programming is easy in <span
class="math inline">\(C\)</span></p>
<p>Choices for <span class="math inline">\(\alpha_k\)</span>: -
diminishing step size: <span class="math inline">\(\alpha_k =
2/(k+1)\)</span> - or via exact line search</p>
<h2 id="lecture-9-prox-mapping">Lecture 9: Prox Mapping</h2>
<h3 id="definition-of-proximal-mapping">Definition of Proximal
Mapping</h3>
<p><strong>Proximal Mapping</strong>: <span class="math display">\[
\text{prox}_{\alpha f}(x) = \arg\min_y \left( f(y) + \frac{1}{2} \|y -
x\|^2 \right)
\]</span></p>
<p><strong>Relation to subgradient</strong>: <span
class="math display">\[
u = \text{prox}_{f}(x) \iff x - u \in \partial f(u)
\]</span></p>
<p><strong>Relation to projection</strong>: <span
class="math display">\[
\text{prox}_{I_C}(x) = \Pi_C(x)
\]</span></p>
<h3 id="moreau-decomposition">Moreau decomposition</h3>
<p><span class="math display">\[
x = \mathrm{prox}_{h}(x) + \mathrm{prox}_{h^*}(x)
\]</span> Generally: <span class="math display">\[
x = \mathrm{prox}_{\lambda h}(x) +
\lambda\mathrm{prox}_{\lambda^{-1}h^*}(x)
\]</span></p>
<p>we can use Moreau decomposition to compute many proximal
mappings.</p>
<p><strong>Norm Ball</strong>: The conjugate of <span
class="math inline">\(\| \cdot \|\)</span> is the indicator function of
the unit ball under the dual norm.</p>
<h3 id="proximal-gradient-descent">Proximal Gradient Descent</h3>
<p><strong>Proximal Gradient Descent</strong>: <span
class="math display">\[
x_{k+1} = \text{prox}_{\alpha h}(x_k - \alpha \nabla f(x_k))
\]</span></p>
<p>due to the optimal condition, we have: <span class="math display">\[
x = \text{prox}_{\alpha h}(x - \alpha \nabla f(x)) \iff -\nabla f(x) \in
\partial h(x)
\]</span></p>
<h4 id="convergency">Convergency</h4>
<p>Define the gradient mapping: <span class="math display">\[
G_t (x) = \frac{1}{t} (x - \text{prox}_{t h}(x - t \nabla f(x)))
\]</span></p>
<p>The proximal gradient descent can be written as: <span
class="math display">\[
x_{k+1} = x_k - t G_{\alpha} (x_k)
\]</span></p>
<p>Convergency for constant step size <span class="math inline">\(\alpha
&lt; 1/L\)</span>: <span class="math display">\[
f(x_k) - f(x^*) \le \frac{2 L}{k}\|x_0 - x^*\|^2
\]</span></p>
<p>Convergency for backtracking, similar. <span class="math display">\[
f(x_k) - f(x^*) \le \frac{1}{2k \min\{ \hat{t}, \beta/L \}} \|x_0 -
x^*\|^2
\]</span></p>
<h3 id="moreau-envelope">Moreau Envelope</h3>
<p><strong>Moreau Envelope</strong>: <span class="math display">\[
h_\alpha (x) = \min_y h(y) + \frac{1}{2\alpha} \|x - y\|^2
\]</span></p>
<p>Moreau envelop can be viewed as an approximation to non smooth <span
class="math inline">\(h\)</span>.</p>
<p>For any proper, closed, convex function, the Moreau envelop <span
class="math inline">\(h_\alpha\)</span> is differentiable, and the
gradient is given by: <span class="math display">\[
\nabla h_\alpha (x) = \frac{1}{\alpha} (x - \text{prox}_{\alpha h}(x))
\]</span></p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a href="/tags/">Tag</a></li>
        
          <li><a href="/categories/">Category</a></li>
        
          <li><a href="/search/">Search</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/adversarr">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#optimization"><span class="toc-number">1.</span> <span class="toc-text">Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-2-convex-set"><span class="toc-number">1.1.</span> <span class="toc-text">Lecture 2: Convex Set</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#common-convex-sets"><span class="toc-number">1.1.1.</span> <span class="toc-text">Common Convex Sets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#property-of-conv"><span class="toc-number">1.1.2.</span> <span class="toc-text">Property of conv</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#operations-on-convex-sets"><span class="toc-number">1.1.3.</span> <span class="toc-text">Operations on Convex Sets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dual-cone-generalized-inequalities"><span class="toc-number">1.1.4.</span> <span class="toc-text">Dual Cone &amp; Generalized
Inequalities</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#separating-hyperplane-theorem"><span class="toc-number">1.1.5.</span> <span class="toc-text">Separating Hyperplane
Theorem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-3-convex-function"><span class="toc-number">1.2.</span> <span class="toc-text">Lecture 3: Convex Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#gradients-hessians"><span class="toc-number">1.2.1.</span> <span class="toc-text">Gradients &amp; Hessians</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#definition"><span class="toc-number">1.2.2.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checking-convexity"><span class="toc-number">1.2.3.</span> <span class="toc-text">checking convexity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#operations"><span class="toc-number">1.2.4.</span> <span class="toc-text">Operations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-4-convex-problems"><span class="toc-number">1.3.</span> <span class="toc-text">Lecture 4: Convex Problems</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-5-optimal-conditions"><span class="toc-number">1.4.</span> <span class="toc-text">Lecture 5: Optimal conditions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#first-order-condition-and-second-order-condition"><span class="toc-number">1.4.1.</span> <span class="toc-text">First Order
Condition and Second Order Condition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#duality"><span class="toc-number">1.4.2.</span> <span class="toc-text">Duality</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimal-condition-slater-kkt-licq"><span class="toc-number">1.4.3.</span> <span class="toc-text">Optimal condition: Slater,
KKT, LICQ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#general-kkt-condition"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">General KKT condition</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-6-gradient-descent"><span class="toc-number">1.5.</span> <span class="toc-text">Lecture 6: Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#more-on-convex-functions"><span class="toc-number">1.5.1.</span> <span class="toc-text">More on convex functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#line-searching"><span class="toc-number">1.5.2.</span> <span class="toc-text">Line searching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#non-smooth-problems"><span class="toc-number">1.5.3.</span> <span class="toc-text">Non smooth problems</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-7-subgradients"><span class="toc-number">1.6.</span> <span class="toc-text">Lecture 7: Subgradients</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-of-subgradients-subdifferential"><span class="toc-number">1.6.1.</span> <span class="toc-text">Definition of
Subgradients &amp; Subdifferential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#computing-subgradients-subdifferential"><span class="toc-number">1.6.2.</span> <span class="toc-text">Computing Subgradients
&amp; Subdifferential</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#optimal-condition"><span class="toc-number">1.6.3.</span> <span class="toc-text">Optimal condition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#subgradient-methods"><span class="toc-number">1.6.4.</span> <span class="toc-text">Subgradient methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-8-projected-gradient-descent-conditional-gradient-descent"><span class="toc-number">1.7.</span> <span class="toc-text">Lecture
8: projected gradient descent, conditional gradient descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-gradient-descent"><span class="toc-number">1.7.1.</span> <span class="toc-text">Conditional Gradient Descent</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#lecture-9-prox-mapping"><span class="toc-number">1.8.</span> <span class="toc-text">Lecture 9: Prox Mapping</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition-of-proximal-mapping"><span class="toc-number">1.8.1.</span> <span class="toc-text">Definition of Proximal
Mapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#moreau-decomposition"><span class="toc-number">1.8.2.</span> <span class="toc-text">Moreau decomposition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#proximal-gradient-descent"><span class="toc-number">1.8.3.</span> <span class="toc-text">Proximal Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#convergency"><span class="toc-number">1.8.3.1.</span> <span class="toc-text">Convergency</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#moreau-envelope"><span class="toc-number">1.8.4.</span> <span class="toc-text">Moreau Envelope</span></a></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&text=A short summary to Optimization Theory."><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&is_video=false&description=A short summary to Optimization Theory."><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=A short summary to Optimization Theory.&body=Check out this article: https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&title=A short summary to Optimization Theory."><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&name=A short summary to Optimization Theory.&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://adversarr.github.io/2024/06/20/optimizer/optimizer_final/&t=A short summary to Optimization Theory."><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2020-2024
    Adversarr
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a href="/tags/">Tag</a></li><!--
     --><!--
       --><li><a href="/categories/">Category</a></li><!--
     --><!--
       --><li><a href="/search/">Search</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/adversarr">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
